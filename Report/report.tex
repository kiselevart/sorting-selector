\documentclass[twocolumn]{article}

\usepackage{graphicx} % Required for inserting images
\usepackage{placeins}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{amssymb}
\usepackage[a4paper, left=0.5in, right=0.5in, top=1in, bottom=1in]{geometry}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{float}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{breakurl}
\usepackage{sectsty}
\usepackage[]{helvet}
\usepackage{titling}
\usepackage{enumitem}
\usepackage{titlesec}

\titlespacing*{\section}{0pt}{1ex}{1ex}

%\allsectionsfont{\sffamily\bfseries}
\pretitle{\begin{center}\sffamily\bfseries\Large}
\posttitle{\par\end{center}}
\preauthor{\begin{center}\sffamily\bfseries}
\postauthor{\par\end{center}}
\predate{\begin{center}\sffamily\bfseries}
\postdate{\par\end{center}}

\usetikzlibrary{automata, positioning, arrows.meta}

\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\newcommand{\vsp}{\vspace{0.5em}}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\prob}{\mathbf{P}}
\newcommand{\ruler}{
    \vspace{1em}
    \hrule
    \vspace{1em}
}
\newcommand{\bsq}{\hfill $\blacksquare$}


\lstset{
    language=C++,          % Set the language for syntax highlighting
    numbers=left,          % Display line numbers on the left
    numberstyle=\tiny\color{gray},  % Style for line numbers
    rulecolor=\color{black},         % Border color around the code block
    basicstyle=\ttfamily\footnotesize, % Set font style and size (smaller font)
    keywordstyle=\color{blue},   % Syntax highlighting for keywords
    commentstyle=\color{green},  % Syntax highlighting for comments
    stringstyle=\color{red},   % Syntax highlighting for strings
    morekeywords={auto, register, inline}, % Add extra C++ keywords if necessary
}

\title{Algorithm Selection: A Predictive Model for Optimal Sorting}
\author{Artem Kiselev}

\begin{document}

\maketitle

\section{Introduction}
Sorting is a fundamental task that appears across various applications in computer science, from database management to data analytics and real-time processing systems. Due to its critical importance, hundreds of sorting algorithms have been developed, each with unique performance characteristics optimized for particular scenarios. Some algorithms excel at sorting nearly-sorted data, others at handling large datasets, and others at optimizing memory usage. As a consequence, no single algorithm universally outperforms all others for all problem instances. This naturally leads to the following research question:

\begin{quote}
\emph{Can we design a model that dynamically and intelligently selects the optimal sorting algorithm based on the characteristics of a given dataset?}
\end{quote}

Successfully addressing this question and constructing an effective predictive model would represent a meaningful advancement. Such a model, capable of analyzing a dataset's characteristics and predicting the optimal sorting strategy based on that analysis, could deliver substantial performance improvements over current static approaches. In practical terms, this could significantly enhance efficiency in real-world scenarios, including large-scale database operations, data-intensive computations, and latency-sensitive applications, providing tangible benefits over existing sorting implementations.

\section{Background and Problem \\ Formalization}
\subsection{Algorithm Selection Problem}
This question is an instance of the algorithm selection problem, formulated by John Rice in 1976 \cite{ricealgorithmselection}. It is formally stated as follows: 

\vsp

\noindent\textbf{Given:}
\begin{itemize}[itemsep=0.05em, topsep=0pt, leftmargin=1em]
    \item A problem space $\mathcal{P}$, containing all possible problem instances.
    \item A feature space $\mathcal{F}$, where each $f(x)\in\mathcal{F}$ represents measurable characteristics of problem $x\in\mathcal{P}$.
    \item An algorithm space $\mathcal{A}$, containing all applicable algorithms $A_i$.
    \item A performance space $\mathbb{R}^n$, where $p(A_i,x)$ represents the performance of algorithm $A_i\in\mathcal{A}$ on problem $x\in\mathcal{P}$.
    \item The final algorithm performance metric $||p||$, obtained by normalizing the raw performance measures.
\end{itemize}

\noindent\textbf{Find:} A selection mapping 
\[
S:\mathcal{F}\rightarrow\mathcal{A}
\]
that maximizes performance according to some criteria.

\subsection{Measures of Presortedness}
Before we discuss how the defined model applies to our research question, we must talk about measures of presortedness, which are a measurement of the pre-existing order of a list. A good measure of presortedness must satisfy the following criteria:

\vsp

\noindent \emph{Let $m$ be a measure of presortedness, and $X, Y$ be lists:}
\begin{enumerate}[itemsep=0.05em, topsep=0pt]
    \item $m(X) = 0$ if $X$ is \emph{fully sorted in ascending order}.
    \item If $X = [x_1, \cdots, x_n]$, $Y= [y_1, \cdots, y_n]$, and 
    \[
    x_i < x_j \iff y_i < y_j \quad \text{for all } i \text{ and } j,
    \]
    then $m(X) = m(Y)$.
    \item If $X \subseteq Y$, then $m(X) \leq m(Y)$.
    \item If $X < Y$, then $m(XY) \leq m(X) + m(Y)$.
    \item \emph{For any element} $a$, $m(a + X) \leq |X| + m(X)$.
\end{enumerate}

For this project, I will explicitly mention the following three measures of presortedness:

\vsp


\noindent \textbf{Runs \cite{knuth1973}:}
The number of maximal contiguous ascending subsequences in a list $X$. 
\[
\text{Runs}(X) = \left|\{i : 1 \leq i < n \text{ and } a_{i+1} < a_i\}\right|.
\]
This takes $O(n)$ time, and $O(1)$ memory.

\ruler

\vsp

\noindent \textbf{Dis \cite{adaptivesortingsurvey}:}
The largest distance for which an inversion exists
\[
\text{Dis}(X) = \operatorname{max}\{j - i : 1 \leq i < j \leq n, x_i > x_j\}.
\]
This takes $O(n)$ time, and $O(n)$ memory.

\ruler

\vsp

\noindent \textbf{Mono \cite{sortingrace2016}:} 
The minimum number $k$ such that $X$ can be broken into $k$ subarrays where each subarray is \emph{monotonic}. A sequence is monotonic if it is increasing \emph{or} decreasing.
\[
\operatorname{Mono}(X)
=
\min\Bigl\{
k \in \mathbb{N}
:\,\exists\,1 = i_{0} < i_{1} < \cdots < i_{k} = n+1 :
\Bigr.
\]
\[
\Bigl.
\forall\, j \in \{1,\dots,k\},
(X_{i_{j-1}},\,X_{i_{j-1}+1},\dots,X_{i_{j}-1})
\text{ is monotonic}
\Bigr\}
\]

This takes $O(n)$ time, and $O(1)$ memory.
\ruler

\vsp

\noindent \textit{Note: }These are the only measures of presortedness that have a linear running time. This quality enables them to be used as features of lists for the problem of sorting.

\subsection{Other List Features}
Aside from the mentioned measures of presortedness, we define five other features:

\begin{enumerate}[itemsep=0.05em, topsep=0pt]
    \item \textbf{Size:} The total number of elements in the list, 
    \[
    \text{Size}(L) = |L|.
    \]
    
    \item \textbf{Average duplicates per unique element:} For a list $L$ with unique elements 
    $U$, if $|L| = n$, then
    \[
    \text{Avg. duplicates} = \frac{n - |U|}{|U|}.
    \]
    
    \item \textbf{Shannon entropy:} With frequency function $f(u)$ for $u\in U$ and $p(u)=\frac{f(u)}{n}$,
    \[
    H(L) = -\sum_{u\in U} p(u)\,\log_2 p(u).
    \]
    
    \item \textbf{Categorical skewness:} For categorical data—where each unique category $u\in U$ occurs with frequency $f(u)$ and the mean $\mu$ and standard deviation $\sigma$ are computed over these frequencies—the skewness is defined as 
    \[
    \gamma_1 = \frac{1}{|U|}\sum_{u\in U}\left(\frac{f(u)-\mu}{\sigma}\right)^3.
    \]
    This measure reflects the asymmetry of the frequency distribution for categorical variables.

    \item \textbf{Categorical kurtosis:} For categorical data with frequencies $f(u)$ for each $u\in U$, the kurtosis is defined by
    \[
    \gamma_2 = \frac{1}{|U|}\sum_{u\in U}\left(\frac{f(u)-\mu}{\sigma}\right)^4.
    \]
    This measure quantifies the tailedness of the frequency distribution for categorical variables.

\end{enumerate}

\subsection{Sorting Algorithm Selection Problem}
In our context, the problem space $\mathcal{P}$ consists of datasets that require sorting, and an instance $x \in \mathcal{P}$ is a list to be sorted. The feature space $\mathcal{F}$ is comprised of the size, categorical skewness, categorical kurtosis, Shannon entropy and, most importantly, the three measures of presortedness Runs, Dis, and Mono.

\vsp

Our algorithm space $\mathcal{A}$ includes various sorting algorithms like QuickSort, MergeSort, InsertionSort, and others. For simplicity, our performance space is defined as $\{0,1\}$, where a value of 1 indicates that the selection algorithm has correctly identified the fastest sorter for a given problem instance. We can now more formally state our research question:

\vsp

\noindent \textbf{Given:}
\begin{itemize}[itemsep=0.05em, topsep=0pt]
    \item Datasets $\mathcal{P} \ni x$, where $x$ is a list that requires sorting.
    \item A feature space $\mathcal{F}$, containing of the above stated features.
    \item An algorithm space $\mathcal{A}$, containing all applicable sorting algorithms $A_i$.
    \item A performance space $\{0,1\}$, where the performance metric is defined as
    \[
    p(A_i, x) = 
    \begin{cases}
    1, & \text{if } A_i \text{ is the fastest sorter for } x, \\
    0, & \text{otherwise.}
    \end{cases}
    \]
\end{itemize}

\noindent \textbf{Find:} A selection mapping
\[
S: \mathcal{F} \rightarrow \mathcal{A}
\]
that maximizes the average number of fastest sorter selections, i.e.,
\[
\max_{S} \frac{1}{|\mathcal{P}|} \sum_{x \in \mathcal{P}} p(S(f(x)), x).
\]

\section{Implementation}
In this section, I reference specific files used to conduct the analysis, all of which can be found in the project's GitHub repository at \href{https://github.com/kiselevart/sorting-selector/}{\texttt{github.com/kiselevart/sorting-selector/}}.

\subsection{cpp-sort Library Integration}
This project relies extensively on the \texttt{cpp-sort}\cite{cppsort_benchmarks} library, a C++ library featuring optimized implementations of various state-of-the-art sorting algorithms. Additionally, \texttt{cpp-sort} provides implementations of most measures of presortedness, referred to as probes.

To effectively leverage \texttt{cpp-sort} within Python, I created two separate interface files using \texttt{pybind11}: one file for interfacing with sorting algorithms and another dedicated to probes. These can be found in the \href{https://github.com/kiselevart/sorting-selector/tree/main/Implementation/src}{\texttt{src/}} directory. Utilizing \texttt{pybind11} allowed seamless integration of high-performance compiled C++ code into Python, thus maintaining Python's advantages in flexibility, extensive support for data science tasks, and ease of use within Jupyter Notebooks.

Before proceeding with the main analysis, I conducted preliminary benchmarks to ensure the reliability and efficiency of \texttt{cpp-sort}'s algorithms. The results from these benchmarks, detailed in \texttt{verify\_running\_time.ipynb}, confirmed that the chosen sorting methods performed consistently and significantly outperformed standard alternatives. Specifically, integrating the compiled sorters from \texttt{cpp-sort} provided, on average, a tenfold speedup compared to NumPy's sorting methods and over a fortyfold improvement compared to Python's built-in sorter.

\subsection{Sorters Redundancy Analysis}

\subsection{Probe Analysis}

\subsection{Machine Learning}

\section{Results}

\section{Discussion}

\section{Conclusion}
This project has made a number of significant 

\bibliographystyle{plain}
\bibliography{references}

\end{document} 